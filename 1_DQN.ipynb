{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, random, pickle, os.path, math, glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import pdb\n",
    "\n",
    "from atari_wrappers import make_atari, wrap_deepmind,LazyFrames\n",
    "from IPython.display import clear_output\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1869658f278>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create and wrap the environment\n",
    "env = make_atari('PongNoFrameskip-v4') # only use in no frameskip environment\n",
    "env = wrap_deepmind(env, scale = False, frame_stack=True )\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "# env.render()\n",
    "test = env.reset()\n",
    "for i in range(100):\n",
    "    test = env.step(env.action_space.sample())[0]\n",
    "\n",
    "plt.imshow(test._force()[...,0])\n",
    "\n",
    "#plt.imshow(env.render(\"rgb_array\"))\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_channels=4, num_actions=5):\n",
    "        \n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.fc5 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
    "        return self.fc5(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory_Buffer(object):\n",
    "    def __init__(self, memory_size=1000):\n",
    "        self.buffer = []\n",
    "        self.memory_size = memory_size\n",
    "        self.next_idx = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        data = (state, action, reward, next_state, done)\n",
    "        if len(self.buffer) <= self.memory_size: # buffer not full\n",
    "            self.buffer.append(data)\n",
    "        else: # buffer is full\n",
    "            self.buffer[self.next_idx] = data\n",
    "        self.next_idx = (self.next_idx + 1) % self.memory_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        for i in range(batch_size):\n",
    "            idx = random.randint(0, self.size() - 1)\n",
    "            data = self.buffer[idx]\n",
    "            state, action, reward, next_state, done= data\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "            \n",
    "            \n",
    "        return np.concatenate(states), actions, rewards, np.concatenate(next_states), dones\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent: \n",
    "    def __init__(self, in_channels = 1, action_space = [], USE_CUDA = False, memory_size = 10000, epsilon  = 1, lr = 1e-4):\n",
    "        self.epsilon = epsilon\n",
    "        self.action_space = action_space\n",
    "        self.memory_buffer = Memory_Buffer(memory_size)\n",
    "        self.DQN = DQN(in_channels = in_channels, num_actions = action_space.n)\n",
    "        self.DQN_target = DQN(in_channels = in_channels, num_actions = action_space.n)\n",
    "        self.DQN_target.load_state_dict(self.DQN.state_dict())\n",
    "\n",
    "\n",
    "        self.USE_CUDA = USE_CUDA\n",
    "        if USE_CUDA:\n",
    "            self.DQN = self.DQN.cuda()\n",
    "            self.DQN_target = self.DQN_target.cuda()\n",
    "        self.optimizer = optim.RMSprop(self.DQN.parameters(),lr=lr, eps=0.001, alpha=0.95)\n",
    "\n",
    "    def observe(self, lazyframe):\n",
    "        # from Lazy frame to tensor\n",
    "        state =  torch.from_numpy(lazyframe._force().transpose(2,0,1)[None]/255).float()\n",
    "        if self.USE_CUDA:\n",
    "            state = state.cuda()\n",
    "        return state\n",
    "\n",
    "    def value(self, state):\n",
    "        q_values = self.DQN(state)\n",
    "        return q_values\n",
    "    \n",
    "    def act(self, state, epsilon = None):\n",
    "        \n",
    "        if epsilon is None: epsilon = self.epsilon\n",
    "\n",
    "        q_values = self.value(state).cpu().detach().numpy()\n",
    "        if random.random()<epsilon:\n",
    "            aciton = random.randrange(self.action_space.n)\n",
    "        else:\n",
    "            aciton = q_values.argmax(1)[0]\n",
    "        return aciton\n",
    "    \n",
    "    def compute_td_loss(self, states, actions, rewards, next_states, is_done, gamma=0.99):\n",
    "        \n",
    "        actions = torch.tensor(actions).long()    # shape: [batch_size]\n",
    "        rewards = torch.tensor(rewards, dtype =torch.float)  # shape: [batch_size]\n",
    "        is_done = torch.tensor(is_done).bool()  # shape: [batch_size]\n",
    "        \n",
    "        if self.USE_CUDA:\n",
    "            actions = actions.cuda()\n",
    "            rewards = rewards.cuda()\n",
    "            is_done = is_done.cuda()\n",
    "\n",
    "        # get q-values for all actions in current states\n",
    "        predicted_qvalues = self.DQN(states)\n",
    "\n",
    "        # select q-values for chosen actions\n",
    "        predicted_qvalues_for_actions = predicted_qvalues[\n",
    "          range(states.shape[0]), actions\n",
    "        ]\n",
    "\n",
    "        # compute q-values for all actions in next states\n",
    "        predicted_next_qvalues = self.DQN_target(next_states) # YOUR CODE\n",
    "\n",
    "        # compute V*(next_states) using predicted next q-values\n",
    "        next_state_values =  predicted_next_qvalues.max(-1)[0] # YOUR CODE\n",
    "\n",
    "        # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "        target_qvalues_for_actions = rewards + gamma *next_state_values # YOUR CODE\n",
    "\n",
    "        # at the last state we shall use simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "        target_qvalues_for_actions = torch.where(\n",
    "            is_done, rewards, target_qvalues_for_actions)\n",
    "\n",
    "        # mean squared error loss to minimize\n",
    "        #loss = torch.mean((predicted_qvalues_for_actions -\n",
    "        #                   target_qvalues_for_actions.detach()) ** 2)\n",
    "        loss = F.smooth_l1_loss(predicted_qvalues_for_actions, target_qvalues_for_actions.detach())\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def sample_from_buffer(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        for i in range(batch_size):\n",
    "            idx = random.randint(0, self.memory_buffer.size() - 1)\n",
    "            data = self.memory_buffer.buffer[idx]\n",
    "            frame, action, reward, next_frame, done= data\n",
    "            states.append(self.observe(frame))\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(self.observe(next_frame))\n",
    "            dones.append(done)\n",
    "        return torch.cat(states), actions, rewards, torch.cat(next_states), dones\n",
    "\n",
    "    def learn_from_experience(self, batch_size):\n",
    "        if self.memory_buffer.size() > batch_size:\n",
    "            states, actions, rewards, next_states, dones = self.sample_from_buffer(batch_size)\n",
    "            td_loss = self.compute_td_loss(states, actions, rewards, next_states, dones)\n",
    "            self.optimizer.zero_grad()\n",
    "            td_loss.backward()\n",
    "            for param in self.DQN.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "            self.optimizer.step()\n",
    "            return(td_loss.item())\n",
    "        else:\n",
    "            return(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wxh20\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\wxh20\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames:     0, reward:   nan, loss: 0.000000, epsilon: 1.000000, episode:    0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames:  1000, reward: -21.000000, loss: 0.000000, epsilon: 0.967544, episode:    1\n",
      "frames:  2000, reward: -20.500000, loss: 0.000000, epsilon: 0.936152, episode:    2\n",
      "frames:  3000, reward: -20.000000, loss: 0.000000, epsilon: 0.905789, episode:    3\n",
      "frames:  4000, reward: -20.250000, loss: 0.000000, epsilon: 0.876422, episode:    4\n",
      "frames:  5000, reward: -20.200000, loss: 0.000000, epsilon: 0.848017, episode:    5\n",
      "frames:  6000, reward: -20.333333, loss: 0.000000, epsilon: 0.820543, episode:    6\n",
      "frames:  7000, reward: -20.285714, loss: 0.000000, epsilon: 0.793971, episode:    7\n",
      "frames:  8000, reward: -20.222222, loss: 0.000000, epsilon: 0.768269, episode:    9\n",
      "frames:  9000, reward: -20.200000, loss: 0.000000, epsilon: 0.743410, episode:   10\n",
      "frames: 10000, reward: -20.200000, loss: 0.001245, epsilon: 0.719366, episode:   11\n",
      "frames: 11000, reward: -20.300000, loss: 0.015207, epsilon: 0.696110, episode:   12\n",
      "frames: 12000, reward: -20.500000, loss: 0.016642, epsilon: 0.673617, episode:   14\n",
      "frames: 13000, reward: -20.600000, loss: 0.015379, epsilon: 0.651861, episode:   15\n",
      "frames: 14000, reward: -20.600000, loss: 0.000329, epsilon: 0.630818, episode:   16\n",
      "frames: 15000, reward: -20.600000, loss: 0.029206, epsilon: 0.610465, episode:   17\n",
      "frames: 16000, reward: -20.600000, loss: 0.000123, epsilon: 0.590780, episode:   18\n",
      "frames: 17000, reward: -20.600000, loss: 0.015611, epsilon: 0.571740, episode:   19\n",
      "frames: 18000, reward: -20.700000, loss: 0.000378, epsilon: 0.553324, episode:   20\n",
      "frames: 19000, reward: -20.600000, loss: 0.000384, epsilon: 0.535511, episode:   21\n",
      "frames: 20000, reward: -20.500000, loss: 0.000606, epsilon: 0.518283, episode:   22\n",
      "frames: 21000, reward: -20.400000, loss: 0.000104, epsilon: 0.501619, episode:   23\n",
      "frames: 22000, reward: -20.400000, loss: 0.015094, epsilon: 0.485502, episode:   24\n",
      "frames: 23000, reward: -20.300000, loss: 0.015641, epsilon: 0.469913, episode:   26\n",
      "frames: 24000, reward: -20.200000, loss: 0.000181, epsilon: 0.454836, episode:   27\n",
      "frames: 25000, reward: -20.300000, loss: 0.045518, epsilon: 0.440252, episode:   28\n",
      "frames: 26000, reward: -20.400000, loss: 0.000123, epsilon: 0.426147, episode:   29\n",
      "frames: 27000, reward: -20.300000, loss: 0.015251, epsilon: 0.412504, episode:   30\n",
      "frames: 28000, reward: -20.400000, loss: 0.000330, epsilon: 0.399308, episode:   31\n",
      "frames: 29000, reward: -20.300000, loss: 0.015312, epsilon: 0.386545, episode:   32\n",
      "frames: 30000, reward: -20.300000, loss: 0.014579, epsilon: 0.374201, episode:   33\n",
      "frames: 31000, reward: -20.200000, loss: 0.015542, epsilon: 0.362261, episode:   34\n",
      "frames: 32000, reward: -20.100000, loss: 0.015026, epsilon: 0.350712, episode:   35\n",
      "frames: 33000, reward: -20.000000, loss: 0.000453, epsilon: 0.339542, episode:   36\n",
      "frames: 34000, reward: -20.000000, loss: 0.000083, epsilon: 0.328739, episode:   38\n",
      "frames: 35000, reward: -19.900000, loss: 0.000292, epsilon: 0.318289, episode:   39\n",
      "frames: 36000, reward: -20.000000, loss: 0.000239, epsilon: 0.308182, episode:   40\n",
      "frames: 37000, reward: -19.900000, loss: 0.000048, epsilon: 0.298407, episode:   41\n",
      "frames: 38000, reward: -20.100000, loss: 0.015612, epsilon: 0.288952, episode:   42\n",
      "frames: 39000, reward: -20.100000, loss: 0.000295, epsilon: 0.279806, episode:   43\n",
      "frames: 40000, reward: -20.000000, loss: 0.015582, epsilon: 0.270961, episode:   45\n",
      "frames: 41000, reward: -20.100000, loss: 0.000127, epsilon: 0.262406, episode:   46\n",
      "frames: 42000, reward: -20.300000, loss: 0.030664, epsilon: 0.254131, episode:   47\n",
      "frames: 43000, reward: -20.200000, loss: 0.009205, epsilon: 0.246127, episode:   48\n",
      "frames: 44000, reward: -20.200000, loss: 0.015103, epsilon: 0.238386, episode:   49\n",
      "frames: 45000, reward: -20.100000, loss: 0.000426, epsilon: 0.230899, episode:   50\n",
      "frames: 46000, reward: -20.100000, loss: 0.000281, epsilon: 0.223657, episode:   52\n",
      "frames: 47000, reward: -20.200000, loss: 0.015754, epsilon: 0.216652, episode:   53\n",
      "frames: 48000, reward: -20.400000, loss: 0.024887, epsilon: 0.209878, episode:   54\n",
      "frames: 49000, reward: -20.600000, loss: 0.015405, epsilon: 0.203325, episode:   55\n",
      "frames: 50000, reward: -20.600000, loss: 0.000306, epsilon: 0.196987, episode:   56\n",
      "frames: 51000, reward: -20.600000, loss: 0.015173, epsilon: 0.190857, episode:   57\n",
      "frames: 52000, reward: -20.500000, loss: 0.000084, epsilon: 0.184928, episode:   58\n",
      "frames: 53000, reward: -20.600000, loss: 0.000522, epsilon: 0.179193, episode:   60\n",
      "frames: 54000, reward: -20.700000, loss: 0.000542, epsilon: 0.173646, episode:   61\n",
      "frames: 55000, reward: -20.600000, loss: 0.000355, epsilon: 0.168281, episode:   62\n",
      "frames: 56000, reward: -20.600000, loss: 0.014163, epsilon: 0.163092, episode:   63\n",
      "frames: 57000, reward: -20.600000, loss: 0.013269, epsilon: 0.158073, episode:   64\n",
      "frames: 58000, reward: -20.600000, loss: 0.040663, epsilon: 0.153219, episode:   65\n",
      "frames: 59000, reward: -20.500000, loss: 0.001196, epsilon: 0.148523, episode:   66\n",
      "frames: 60000, reward: -20.700000, loss: 0.000988, epsilon: 0.143982, episode:   68\n",
      "frames: 61000, reward: -20.700000, loss: 0.000980, epsilon: 0.139589, episode:   69\n",
      "frames: 62000, reward: -20.700000, loss: 0.008921, epsilon: 0.135341, episode:   70\n",
      "frames: 63000, reward: -20.600000, loss: 0.030867, epsilon: 0.131232, episode:   71\n",
      "frames: 64000, reward: -20.500000, loss: 0.009127, epsilon: 0.127257, episode:   72\n",
      "frames: 65000, reward: -20.500000, loss: 0.009686, epsilon: 0.123413, episode:   73\n",
      "frames: 66000, reward: -20.500000, loss: 0.004359, epsilon: 0.119695, episode:   74\n",
      "frames: 67000, reward: -20.500000, loss: 0.004195, epsilon: 0.116099, episode:   76\n",
      "frames: 68000, reward: -20.500000, loss: 0.010518, epsilon: 0.112621, episode:   77\n",
      "frames: 69000, reward: -20.500000, loss: 0.014100, epsilon: 0.109256, episode:   78\n",
      "frames: 70000, reward: -20.600000, loss: 0.003201, epsilon: 0.106002, episode:   79\n",
      "frames: 71000, reward: -20.600000, loss: 0.000855, epsilon: 0.102855, episode:   80\n",
      "frames: 72000, reward: -20.900000, loss: 0.005903, epsilon: 0.099811, episode:   82\n",
      "frames: 73000, reward: -20.700000, loss: 0.002000, epsilon: 0.096866, episode:   83\n",
      "frames: 74000, reward: -20.600000, loss: 0.007633, epsilon: 0.094019, episode:   84\n",
      "frames: 75000, reward: -20.600000, loss: 0.000960, epsilon: 0.091264, episode:   85\n",
      "frames: 76000, reward: -20.600000, loss: 0.003780, epsilon: 0.088600, episode:   86\n",
      "frames: 77000, reward: -20.500000, loss: 0.001865, epsilon: 0.086023, episode:   87\n",
      "frames: 78000, reward: -20.200000, loss: 0.000759, epsilon: 0.083531, episode:   88\n",
      "frames: 79000, reward: -20.100000, loss: 0.002311, epsilon: 0.081120, episode:   89\n",
      "frames: 80000, reward: -20.100000, loss: 0.010811, epsilon: 0.078789, episode:   90\n",
      "frames: 81000, reward: -20.100000, loss: 0.001028, epsilon: 0.076533, episode:   91\n",
      "frames: 82000, reward: -20.100000, loss: 0.002205, epsilon: 0.074352, episode:   92\n",
      "frames: 83000, reward: -20.300000, loss: 0.003848, epsilon: 0.072243, episode:   93\n",
      "frames: 84000, reward: -20.300000, loss: 0.002709, epsilon: 0.070202, episode:   94\n",
      "frames: 85000, reward: -20.400000, loss: 0.001131, epsilon: 0.068228, episode:   96\n",
      "frames: 86000, reward: -20.500000, loss: 0.001849, epsilon: 0.066319, episode:   97\n",
      "frames: 87000, reward: -20.800000, loss: 0.001626, epsilon: 0.064473, episode:   98\n",
      "frames: 88000, reward: -20.800000, loss: 0.000675, epsilon: 0.062687, episode:   99\n",
      "frames: 89000, reward: -20.800000, loss: 0.000874, epsilon: 0.060960, episode:  101\n",
      "frames: 90000, reward: -20.800000, loss: 0.000628, epsilon: 0.059289, episode:  102\n",
      "frames: 91000, reward: -20.800000, loss: 0.000979, epsilon: 0.057673, episode:  103\n",
      "frames: 92000, reward: -20.900000, loss: 0.001028, epsilon: 0.056110, episode:  104\n",
      "frames: 93000, reward: -20.900000, loss: 0.000530, epsilon: 0.054599, episode:  105\n",
      "frames: 94000, reward: -20.800000, loss: 0.001432, epsilon: 0.053137, episode:  106\n",
      "frames: 95000, reward: -20.800000, loss: 0.002387, epsilon: 0.051722, episode:  108\n",
      "frames: 96000, reward: -20.900000, loss: 0.004245, epsilon: 0.050355, episode:  109\n",
      "frames: 97000, reward: -20.800000, loss: 0.001495, epsilon: 0.049032, episode:  110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames: 98000, reward: -20.800000, loss: 0.000452, epsilon: 0.047752, episode:  111\n",
      "frames: 99000, reward: -20.700000, loss: 0.002496, epsilon: 0.046514, episode:  112\n",
      "frames: 100000, reward: -20.600000, loss: 0.008964, epsilon: 0.045317, episode:  113\n",
      "frames: 101000, reward: -20.500000, loss: 0.003415, epsilon: 0.044159, episode:  114\n",
      "frames: 102000, reward: -20.500000, loss: 0.004007, epsilon: 0.043040, episode:  115\n",
      "frames: 103000, reward: -20.400000, loss: 0.001225, epsilon: 0.041956, episode:  116\n",
      "frames: 104000, reward: -20.400000, loss: 0.005211, epsilon: 0.040909, episode:  117\n",
      "frames: 105000, reward: -20.300000, loss: 0.000879, epsilon: 0.039895, episode:  118\n",
      "frames: 106000, reward: -20.400000, loss: 0.002207, epsilon: 0.038915, episode:  120\n",
      "frames: 107000, reward: -20.200000, loss: 0.001388, epsilon: 0.037967, episode:  121\n",
      "frames: 108000, reward: -20.300000, loss: 0.002390, epsilon: 0.037050, episode:  122\n",
      "frames: 109000, reward: -20.300000, loss: 0.002864, epsilon: 0.036164, episode:  123\n",
      "frames: 110000, reward: -20.400000, loss: 0.001401, epsilon: 0.035306, episode:  124\n",
      "frames: 111000, reward: -20.300000, loss: 0.002095, epsilon: 0.034476, episode:  125\n",
      "frames: 112000, reward: -20.500000, loss: 0.003601, epsilon: 0.033674, episode:  126\n",
      "frames: 113000, reward: -20.300000, loss: 0.001387, epsilon: 0.032898, episode:  127\n",
      "frames: 114000, reward: -20.100000, loss: 0.003408, epsilon: 0.032147, episode:  128\n",
      "frames: 115000, reward: -20.000000, loss: 0.001761, epsilon: 0.031421, episode:  129\n",
      "frames: 116000, reward: -20.000000, loss: 0.001370, epsilon: 0.030719, episode:  129\n",
      "frames: 117000, reward: -19.700000, loss: 0.002047, epsilon: 0.030039, episode:  130\n",
      "frames: 118000, reward: -19.600000, loss: 0.002873, epsilon: 0.029383, episode:  131\n",
      "frames: 119000, reward: -19.600000, loss: 0.003753, epsilon: 0.028747, episode:  132\n",
      "frames: 120000, reward: -19.500000, loss: 0.000916, epsilon: 0.028132, episode:  133\n",
      "frames: 121000, reward: -19.500000, loss: 0.002134, epsilon: 0.027538, episode:  134\n",
      "frames: 122000, reward: -19.600000, loss: 0.002869, epsilon: 0.026963, episode:  135\n",
      "frames: 123000, reward: -19.500000, loss: 0.005174, epsilon: 0.026407, episode:  136\n",
      "frames: 124000, reward: -19.700000, loss: 0.003875, epsilon: 0.025869, episode:  137\n",
      "frames: 125000, reward: -19.700000, loss: 0.005169, epsilon: 0.025349, episode:  137\n",
      "frames: 126000, reward: -19.900000, loss: 0.004996, epsilon: 0.024846, episode:  138\n",
      "frames: 127000, reward: -19.900000, loss: 0.003926, epsilon: 0.024359, episode:  139\n",
      "frames: 128000, reward: -20.100000, loss: 0.007446, epsilon: 0.023888, episode:  140\n",
      "frames: 129000, reward: -20.100000, loss: 0.002684, epsilon: 0.023433, episode:  140\n",
      "frames: 130000, reward: -20.300000, loss: 0.004589, epsilon: 0.022992, episode:  141\n",
      "frames: 131000, reward: -20.300000, loss: 0.003576, epsilon: 0.022567, episode:  142\n",
      "frames: 132000, reward: -20.300000, loss: 0.001674, epsilon: 0.022155, episode:  142\n",
      "frames: 133000, reward: -20.500000, loss: 0.001630, epsilon: 0.021756, episode:  143\n",
      "frames: 134000, reward: -20.400000, loss: 0.002025, epsilon: 0.021371, episode:  144\n",
      "frames: 135000, reward: -20.400000, loss: 0.000936, epsilon: 0.020998, episode:  144\n",
      "frames: 136000, reward: -20.400000, loss: 0.001779, epsilon: 0.020637, episode:  145\n",
      "frames: 137000, reward: -20.100000, loss: 0.003275, epsilon: 0.020289, episode:  146\n",
      "frames: 138000, reward: -20.100000, loss: 0.002817, epsilon: 0.019951, episode:  146\n",
      "frames: 139000, reward: -20.000000, loss: 0.001012, epsilon: 0.019625, episode:  147\n",
      "frames: 140000, reward: -20.000000, loss: 0.002706, epsilon: 0.019310, episode:  148\n",
      "frames: 141000, reward: -20.000000, loss: 0.003392, epsilon: 0.019004, episode:  149\n",
      "frames: 142000, reward: -20.000000, loss: 0.001293, epsilon: 0.018709, episode:  149\n",
      "frames: 143000, reward: -20.000000, loss: 0.004169, epsilon: 0.018424, episode:  150\n",
      "frames: 144000, reward: -20.000000, loss: 0.002386, epsilon: 0.018147, episode:  150\n",
      "frames: 145000, reward: -19.800000, loss: 0.002442, epsilon: 0.017880, episode:  151\n",
      "frames: 146000, reward: -19.800000, loss: 0.002600, epsilon: 0.017622, episode:  151\n",
      "frames: 147000, reward: -19.800000, loss: 0.002200, epsilon: 0.017372, episode:  151\n",
      "frames: 148000, reward: -18.200000, loss: 0.002105, epsilon: 0.017130, episode:  152\n",
      "frames: 149000, reward: -18.100000, loss: 0.001214, epsilon: 0.016897, episode:  153\n",
      "frames: 150000, reward: -18.100000, loss: 0.001188, epsilon: 0.016671, episode:  154\n",
      "frames: 151000, reward: -17.800000, loss: 0.001762, epsilon: 0.016452, episode:  155\n",
      "frames: 152000, reward: -18.200000, loss: 0.002290, epsilon: 0.016240, episode:  156\n",
      "frames: 153000, reward: -18.200000, loss: 0.002202, epsilon: 0.016036, episode:  156\n",
      "frames: 154000, reward: -18.000000, loss: 0.006023, epsilon: 0.015838, episode:  157\n",
      "frames: 155000, reward: -17.900000, loss: 0.000778, epsilon: 0.015647, episode:  158\n",
      "frames: 156000, reward: -17.900000, loss: 0.016053, epsilon: 0.015461, episode:  159\n",
      "frames: 157000, reward: -17.900000, loss: 0.004846, epsilon: 0.015282, episode:  159\n",
      "frames: 158000, reward: -17.800000, loss: 0.001574, epsilon: 0.015109, episode:  160\n",
      "frames: 159000, reward: -17.800000, loss: 0.004294, epsilon: 0.014942, episode:  160\n",
      "frames: 160000, reward: -17.600000, loss: 0.009022, epsilon: 0.014780, episode:  161\n",
      "frames: 161000, reward: -19.000000, loss: 0.003068, epsilon: 0.014623, episode:  162\n",
      "frames: 162000, reward: -19.000000, loss: 0.002518, epsilon: 0.014471, episode:  162\n",
      "frames: 163000, reward: -18.900000, loss: 0.001831, epsilon: 0.014325, episode:  163\n",
      "frames: 164000, reward: -18.800000, loss: 0.001798, epsilon: 0.014183, episode:  164\n",
      "frames: 165000, reward: -18.800000, loss: 0.001515, epsilon: 0.014046, episode:  164\n",
      "frames: 166000, reward: -18.800000, loss: 0.003814, epsilon: 0.013913, episode:  165\n",
      "frames: 167000, reward: -18.800000, loss: 0.002778, epsilon: 0.013785, episode:  165\n",
      "frames: 168000, reward: -18.800000, loss: 0.003354, epsilon: 0.013661, episode:  166\n",
      "frames: 169000, reward: -18.900000, loss: 0.002291, epsilon: 0.013541, episode:  167\n",
      "frames: 170000, reward: -18.900000, loss: 0.001638, epsilon: 0.013425, episode:  167\n",
      "frames: 171000, reward: -19.000000, loss: 0.002592, epsilon: 0.013313, episode:  168\n",
      "frames: 172000, reward: -19.000000, loss: 0.005045, epsilon: 0.013204, episode:  168\n",
      "frames: 173000, reward: -18.900000, loss: 0.002737, epsilon: 0.013099, episode:  169\n",
      "frames: 174000, reward: -18.900000, loss: 0.001523, epsilon: 0.012997, episode:  169\n",
      "frames: 175000, reward: -18.900000, loss: 0.002039, epsilon: 0.012899, episode:  170\n",
      "frames: 176000, reward: -19.300000, loss: 0.001101, epsilon: 0.012804, episode:  171\n",
      "frames: 177000, reward: -19.300000, loss: 0.002230, epsilon: 0.012712, episode:  171\n",
      "frames: 178000, reward: -19.000000, loss: 0.001977, epsilon: 0.012623, episode:  172\n",
      "frames: 179000, reward: -19.000000, loss: 0.001678, epsilon: 0.012537, episode:  172\n",
      "frames: 180000, reward: -19.000000, loss: 0.001054, epsilon: 0.012454, episode:  172\n",
      "frames: 181000, reward: -18.900000, loss: 0.003355, epsilon: 0.012374, episode:  173\n",
      "frames: 182000, reward: -18.900000, loss: 0.004114, epsilon: 0.012296, episode:  173\n",
      "frames: 183000, reward: -18.900000, loss: 0.003263, epsilon: 0.012220, episode:  174\n",
      "frames: 184000, reward: -18.900000, loss: 0.001051, epsilon: 0.012148, episode:  174\n",
      "frames: 185000, reward: -18.800000, loss: 0.004558, epsilon: 0.012077, episode:  175\n",
      "frames: 186000, reward: -18.800000, loss: 0.001115, epsilon: 0.012009, episode:  175\n",
      "frames: 187000, reward: -18.800000, loss: 0.001405, epsilon: 0.011943, episode:  175\n",
      "frames: 188000, reward: -18.200000, loss: 0.001845, epsilon: 0.011880, episode:  176\n",
      "frames: 189000, reward: -18.200000, loss: 0.000894, epsilon: 0.011818, episode:  176\n",
      "frames: 190000, reward: -18.200000, loss: 0.001312, epsilon: 0.011758, episode:  176\n",
      "frames: 191000, reward: -17.700000, loss: 0.001515, epsilon: 0.011701, episode:  177\n",
      "frames: 192000, reward: -17.700000, loss: 0.001230, epsilon: 0.011645, episode:  177\n",
      "frames: 193000, reward: -17.100000, loss: 0.001939, epsilon: 0.011591, episode:  178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames: 194000, reward: -17.100000, loss: 0.007990, epsilon: 0.011539, episode:  178\n",
      "frames: 195000, reward: -17.100000, loss: 0.002221, epsilon: 0.011488, episode:  179\n",
      "frames: 196000, reward: -17.100000, loss: 0.001793, epsilon: 0.011440, episode:  179\n",
      "frames: 197000, reward: -16.700000, loss: 0.004036, epsilon: 0.011392, episode:  180\n",
      "frames: 198000, reward: -16.700000, loss: 0.001382, epsilon: 0.011347, episode:  180\n",
      "frames: 199000, reward: -16.700000, loss: 0.000952, epsilon: 0.011303, episode:  180\n",
      "frames: 200000, reward: -16.300000, loss: 0.003129, epsilon: 0.011260, episode:  181\n",
      "frames: 201000, reward: -16.300000, loss: 0.004046, epsilon: 0.011219, episode:  181\n",
      "frames: 202000, reward: -16.300000, loss: 0.001576, epsilon: 0.011179, episode:  181\n",
      "frames: 203000, reward: -15.400000, loss: 0.001453, epsilon: 0.011140, episode:  182\n",
      "frames: 204000, reward: -15.400000, loss: 0.002732, epsilon: 0.011103, episode:  182\n",
      "frames: 205000, reward: -15.000000, loss: 0.001640, epsilon: 0.011066, episode:  183\n",
      "frames: 206000, reward: -15.000000, loss: 0.002369, epsilon: 0.011032, episode:  183\n",
      "frames: 207000, reward: -15.000000, loss: 0.001677, epsilon: 0.010998, episode:  183\n",
      "frames: 208000, reward: -14.500000, loss: 0.008033, epsilon: 0.010965, episode:  184\n",
      "frames: 209000, reward: -14.500000, loss: 0.003483, epsilon: 0.010933, episode:  184\n",
      "frames: 210000, reward: -14.500000, loss: 0.000922, epsilon: 0.010903, episode:  184\n",
      "frames: 211000, reward: -14.600000, loss: 0.002025, epsilon: 0.010873, episode:  185\n",
      "frames: 212000, reward: -14.600000, loss: 0.001815, epsilon: 0.010845, episode:  185\n",
      "frames: 213000, reward: -14.600000, loss: 0.002451, epsilon: 0.010817, episode:  185\n",
      "frames: 214000, reward: -14.800000, loss: 0.001265, epsilon: 0.010790, episode:  186\n",
      "frames: 215000, reward: -14.800000, loss: 0.002329, epsilon: 0.010764, episode:  186\n",
      "frames: 216000, reward: -14.800000, loss: 0.007190, epsilon: 0.010739, episode:  186\n",
      "frames: 217000, reward: -14.400000, loss: 0.000973, epsilon: 0.010715, episode:  187\n",
      "frames: 218000, reward: -14.400000, loss: 0.003418, epsilon: 0.010691, episode:  187\n",
      "frames: 219000, reward: -14.500000, loss: 0.002199, epsilon: 0.010669, episode:  188\n",
      "frames: 220000, reward: -14.500000, loss: 0.001142, epsilon: 0.010647, episode:  188\n",
      "frames: 221000, reward: -14.500000, loss: 0.001454, epsilon: 0.010626, episode:  189\n",
      "frames: 222000, reward: -14.500000, loss: 0.001649, epsilon: 0.010605, episode:  189\n",
      "frames: 223000, reward: -14.500000, loss: 0.001095, epsilon: 0.010585, episode:  189\n",
      "frames: 224000, reward: -12.700000, loss: 0.001491, epsilon: 0.010566, episode:  190\n",
      "frames: 225000, reward: -12.700000, loss: 0.001341, epsilon: 0.010548, episode:  190\n",
      "frames: 226000, reward: -12.700000, loss: 0.001504, epsilon: 0.010530, episode:  190\n",
      "frames: 227000, reward: -10.600000, loss: 0.000502, epsilon: 0.010512, episode:  191\n",
      "frames: 228000, reward: -10.600000, loss: 0.001727, epsilon: 0.010495, episode:  191\n",
      "frames: 229000, reward: -11.100000, loss: 0.002600, epsilon: 0.010479, episode:  192\n",
      "frames: 230000, reward: -11.100000, loss: 0.002037, epsilon: 0.010463, episode:  192\n",
      "frames: 231000, reward: -11.100000, loss: 0.007590, epsilon: 0.010448, episode:  192\n",
      "frames: 232000, reward: -9.200000, loss: 0.001203, epsilon: 0.010434, episode:  193\n",
      "frames: 233000, reward: -9.200000, loss: 0.001748, epsilon: 0.010419, episode:  193\n",
      "frames: 234000, reward: -9.200000, loss: 0.001848, epsilon: 0.010406, episode:  193\n",
      "frames: 235000, reward: -7.000000, loss: 0.002251, epsilon: 0.010392, episode:  194\n",
      "frames: 236000, reward: -7.000000, loss: 0.003490, epsilon: 0.010379, episode:  194\n",
      "frames: 237000, reward: -6.100000, loss: 0.003639, epsilon: 0.010367, episode:  195\n",
      "frames: 238000, reward: -6.100000, loss: 0.000918, epsilon: 0.010355, episode:  195\n",
      "frames: 239000, reward: -6.100000, loss: 0.001767, epsilon: 0.010343, episode:  195\n",
      "frames: 240000, reward: -5.200000, loss: 0.001009, epsilon: 0.010332, episode:  196\n",
      "frames: 241000, reward: -2.400000, loss: 0.001650, epsilon: 0.010321, episode:  197\n",
      "frames: 242000, reward: -2.400000, loss: 0.000786, epsilon: 0.010311, episode:  197\n",
      "frames: 243000, reward: -2.400000, loss: 0.001421, epsilon: 0.010301, episode:  197\n",
      "frames: 244000, reward: -2.100000, loss: 0.001162, epsilon: 0.010291, episode:  198\n",
      "frames: 245000, reward: -2.100000, loss: 0.001760, epsilon: 0.010281, episode:  198\n",
      "frames: 246000, reward: 0.300000, loss: 0.001411, epsilon: 0.010272, episode:  199\n",
      "frames: 247000, reward: 0.300000, loss: 0.001708, epsilon: 0.010263, episode:  199\n",
      "frames: 248000, reward: 0.300000, loss: 0.001411, epsilon: 0.010254, episode:  199\n",
      "frames: 249000, reward: 0.800000, loss: 0.001603, epsilon: 0.010246, episode:  200\n",
      "frames: 250000, reward: -1.800000, loss: 0.001494, epsilon: 0.010238, episode:  201\n",
      "frames: 251000, reward: -1.800000, loss: 0.001441, epsilon: 0.010230, episode:  201\n",
      "frames: 252000, reward: 1.100000, loss: 0.001093, epsilon: 0.010223, episode:  202\n",
      "frames: 253000, reward: -1.000000, loss: 0.000568, epsilon: 0.010215, episode:  203\n",
      "frames: 254000, reward: -1.000000, loss: 0.001128, epsilon: 0.010208, episode:  203\n",
      "frames: 255000, reward: -3.000000, loss: 0.004369, epsilon: 0.010201, episode:  204\n",
      "frames: 256000, reward: -3.000000, loss: 0.001805, epsilon: 0.010195, episode:  204\n",
      "frames: 257000, reward: -0.400000, loss: 0.003961, epsilon: 0.010188, episode:  205\n",
      "frames: 258000, reward: -0.400000, loss: 0.001065, epsilon: 0.010182, episode:  205\n",
      "frames: 259000, reward: -0.400000, loss: 0.000878, epsilon: 0.010176, episode:  205\n",
      "frames: 260000, reward: 0.800000, loss: 0.001266, epsilon: 0.010171, episode:  206\n",
      "frames: 261000, reward: 0.800000, loss: 0.000785, epsilon: 0.010165, episode:  206\n",
      "frames: 262000, reward: 1.000000, loss: 0.001317, epsilon: 0.010160, episode:  207\n",
      "frames: 263000, reward: 1.000000, loss: 0.001207, epsilon: 0.010154, episode:  207\n",
      "frames: 264000, reward: 1.000000, loss: 0.000792, epsilon: 0.010149, episode:  207\n",
      "frames: 265000, reward: 2.100000, loss: 0.001498, epsilon: 0.010144, episode:  208\n",
      "frames: 266000, reward: 3.400000, loss: 0.001669, epsilon: 0.010140, episode:  209\n",
      "frames: 267000, reward: 3.400000, loss: 0.001837, epsilon: 0.010135, episode:  209\n",
      "frames: 268000, reward: 3.400000, loss: 0.000744, epsilon: 0.010131, episode:  209\n",
      "frames: 269000, reward: 3.400000, loss: 0.001000, epsilon: 0.010126, episode:  210\n",
      "frames: 270000, reward: 3.400000, loss: 0.001083, epsilon: 0.010122, episode:  210\n",
      "frames: 271000, reward: 3.400000, loss: 0.000964, epsilon: 0.010118, episode:  210\n",
      "frames: 272000, reward: 5.600000, loss: 0.000892, epsilon: 0.010114, episode:  211\n",
      "frames: 273000, reward: 5.600000, loss: 0.002252, epsilon: 0.010111, episode:  211\n",
      "frames: 274000, reward: 5.600000, loss: 0.001212, epsilon: 0.010107, episode:  211\n",
      "frames: 275000, reward: 4.200000, loss: 0.000690, epsilon: 0.010103, episode:  212\n",
      "frames: 276000, reward: 4.200000, loss: 0.000578, epsilon: 0.010100, episode:  212\n",
      "frames: 277000, reward: 4.200000, loss: 0.001326, epsilon: 0.010097, episode:  212\n",
      "frames: 278000, reward: 6.000000, loss: 0.002997, epsilon: 0.010094, episode:  213\n",
      "frames: 279000, reward: 6.000000, loss: 0.000837, epsilon: 0.010091, episode:  213\n",
      "frames: 280000, reward: 9.100000, loss: 0.003935, epsilon: 0.010088, episode:  214\n",
      "frames: 281000, reward: 9.100000, loss: 0.001643, epsilon: 0.010085, episode:  214\n",
      "frames: 282000, reward: 9.400000, loss: 0.000936, epsilon: 0.010082, episode:  215\n",
      "frames: 283000, reward: 9.400000, loss: 0.001437, epsilon: 0.010079, episode:  215\n",
      "frames: 284000, reward: 10.300000, loss: 0.000817, epsilon: 0.010077, episode:  216\n",
      "frames: 285000, reward: 10.300000, loss: 0.001273, epsilon: 0.010074, episode:  216\n",
      "frames: 286000, reward: 10.300000, loss: 0.000548, epsilon: 0.010072, episode:  216\n",
      "frames: 287000, reward: 7.900000, loss: 0.001247, epsilon: 0.010069, episode:  217\n",
      "frames: 288000, reward: 7.900000, loss: 0.000470, epsilon: 0.010067, episode:  217\n",
      "frames: 289000, reward: 9.500000, loss: 0.000907, epsilon: 0.010065, episode:  218\n",
      "frames: 290000, reward: 9.500000, loss: 0.002975, epsilon: 0.010063, episode:  218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames: 291000, reward: 9.500000, loss: 0.000828, epsilon: 0.010061, episode:  219\n",
      "frames: 292000, reward: 10.200000, loss: 0.001176, epsilon: 0.010059, episode:  220\n",
      "frames: 293000, reward: 10.200000, loss: 0.000623, epsilon: 0.010057, episode:  220\n",
      "frames: 294000, reward: 10.200000, loss: 0.004566, epsilon: 0.010055, episode:  220\n",
      "frames: 295000, reward: 9.500000, loss: 0.000843, epsilon: 0.010053, episode:  221\n",
      "frames: 296000, reward: 9.500000, loss: 0.001914, epsilon: 0.010051, episode:  221\n",
      "frames: 297000, reward: 8.200000, loss: 0.002742, epsilon: 0.010050, episode:  222\n",
      "frames: 298000, reward: 8.200000, loss: 0.000944, epsilon: 0.010048, episode:  222\n",
      "frames: 299000, reward: 9.800000, loss: 0.000496, epsilon: 0.010046, episode:  223\n",
      "frames: 300000, reward: 9.800000, loss: 0.000670, epsilon: 0.010045, episode:  223\n",
      "frames: 301000, reward: 9.800000, loss: 0.001513, epsilon: 0.010043, episode:  224\n",
      "frames: 302000, reward: 9.800000, loss: 0.001168, epsilon: 0.010042, episode:  224\n",
      "frames: 303000, reward: 9.200000, loss: 0.001142, epsilon: 0.010041, episode:  225\n",
      "frames: 304000, reward: 9.200000, loss: 0.000713, epsilon: 0.010039, episode:  225\n",
      "frames: 305000, reward: 9.900000, loss: 0.000819, epsilon: 0.010038, episode:  226\n",
      "frames: 306000, reward: 9.900000, loss: 0.000534, epsilon: 0.010037, episode:  226\n",
      "frames: 307000, reward: 12.300000, loss: 0.003247, epsilon: 0.010036, episode:  227\n",
      "frames: 308000, reward: 12.300000, loss: 0.000318, epsilon: 0.010034, episode:  227\n",
      "frames: 309000, reward: 12.200000, loss: 0.000642, epsilon: 0.010033, episode:  228\n",
      "frames: 310000, reward: 12.200000, loss: 0.000502, epsilon: 0.010032, episode:  228\n",
      "frames: 311000, reward: 11.700000, loss: 0.001788, epsilon: 0.010031, episode:  229\n",
      "frames: 312000, reward: 11.700000, loss: 0.001152, epsilon: 0.010030, episode:  229\n",
      "frames: 313000, reward: 11.600000, loss: 0.000873, epsilon: 0.010029, episode:  230\n",
      "frames: 314000, reward: 11.600000, loss: 0.000573, epsilon: 0.010028, episode:  230\n",
      "frames: 315000, reward: 13.900000, loss: 0.001497, epsilon: 0.010027, episode:  231\n",
      "frames: 316000, reward: 13.900000, loss: 0.001941, epsilon: 0.010026, episode:  231\n",
      "frames: 317000, reward: 13.900000, loss: 0.000334, epsilon: 0.010026, episode:  231\n",
      "frames: 318000, reward: 15.500000, loss: 0.000586, epsilon: 0.010025, episode:  232\n",
      "frames: 319000, reward: 15.500000, loss: 0.000559, epsilon: 0.010024, episode:  232\n",
      "frames: 320000, reward: 15.700000, loss: 0.001494, epsilon: 0.010023, episode:  233\n",
      "frames: 321000, reward: 15.700000, loss: 0.000531, epsilon: 0.010022, episode:  233\n",
      "frames: 322000, reward: 15.600000, loss: 0.000725, epsilon: 0.010022, episode:  234\n",
      "frames: 323000, reward: 15.600000, loss: 0.000561, epsilon: 0.010021, episode:  234\n",
      "frames: 324000, reward: 15.500000, loss: 0.000573, epsilon: 0.010020, episode:  235\n",
      "frames: 325000, reward: 15.500000, loss: 0.000548, epsilon: 0.010020, episode:  235\n",
      "frames: 326000, reward: 15.200000, loss: 0.001781, epsilon: 0.010019, episode:  236\n",
      "frames: 327000, reward: 15.200000, loss: 0.000406, epsilon: 0.010018, episode:  236\n",
      "frames: 328000, reward: 15.200000, loss: 0.001491, epsilon: 0.010018, episode:  236\n",
      "frames: 329000, reward: 14.500000, loss: 0.001021, epsilon: 0.010017, episode:  237\n",
      "frames: 330000, reward: 14.500000, loss: 0.001327, epsilon: 0.010017, episode:  237\n",
      "frames: 331000, reward: 14.500000, loss: 0.000621, epsilon: 0.010016, episode:  237\n",
      "frames: 332000, reward: 13.900000, loss: 0.000929, epsilon: 0.010015, episode:  238\n",
      "frames: 333000, reward: 13.900000, loss: 0.002903, epsilon: 0.010015, episode:  238\n",
      "frames: 334000, reward: 14.300000, loss: 0.001140, epsilon: 0.010014, episode:  239\n",
      "frames: 335000, reward: 14.300000, loss: 0.000733, epsilon: 0.010014, episode:  239\n",
      "frames: 336000, reward: 13.700000, loss: 0.001180, epsilon: 0.010014, episode:  240\n",
      "frames: 337000, reward: 13.700000, loss: 0.001640, epsilon: 0.010013, episode:  240\n",
      "frames: 338000, reward: 13.700000, loss: 0.001214, epsilon: 0.010013, episode:  240\n",
      "frames: 339000, reward: 13.300000, loss: 0.001096, epsilon: 0.010012, episode:  241\n",
      "frames: 340000, reward: 13.300000, loss: 0.000312, epsilon: 0.010012, episode:  241\n",
      "frames: 341000, reward: 14.700000, loss: 0.000558, epsilon: 0.010011, episode:  242\n",
      "frames: 342000, reward: 14.700000, loss: 0.002414, epsilon: 0.010011, episode:  242\n",
      "frames: 343000, reward: 14.500000, loss: 0.000430, epsilon: 0.010011, episode:  243\n",
      "frames: 344000, reward: 14.500000, loss: 0.000708, epsilon: 0.010010, episode:  243\n",
      "frames: 345000, reward: 14.500000, loss: 0.000334, epsilon: 0.010010, episode:  243\n",
      "frames: 346000, reward: 13.300000, loss: 0.000501, epsilon: 0.010010, episode:  244\n",
      "frames: 347000, reward: 13.300000, loss: 0.001024, epsilon: 0.010009, episode:  244\n",
      "frames: 348000, reward: 13.300000, loss: 0.000753, epsilon: 0.010009, episode:  244\n",
      "frames: 349000, reward: 14.000000, loss: 0.000605, epsilon: 0.010009, episode:  245\n",
      "frames: 350000, reward: 14.000000, loss: 0.001172, epsilon: 0.010008, episode:  245\n",
      "frames: 351000, reward: 13.900000, loss: 0.000864, epsilon: 0.010008, episode:  246\n",
      "frames: 352000, reward: 13.900000, loss: 0.000596, epsilon: 0.010008, episode:  246\n",
      "frames: 353000, reward: 13.900000, loss: 0.000955, epsilon: 0.010008, episode:  246\n",
      "frames: 354000, reward: 13.200000, loss: 0.001212, epsilon: 0.010007, episode:  247\n",
      "frames: 355000, reward: 13.200000, loss: 0.001657, epsilon: 0.010007, episode:  247\n",
      "frames: 356000, reward: 13.200000, loss: 0.000742, epsilon: 0.010007, episode:  247\n",
      "frames: 357000, reward: 12.300000, loss: 0.001013, epsilon: 0.010007, episode:  248\n",
      "frames: 358000, reward: 12.300000, loss: 0.000872, epsilon: 0.010007, episode:  248\n",
      "frames: 359000, reward: 12.300000, loss: 0.000433, epsilon: 0.010006, episode:  248\n",
      "frames: 360000, reward: 12.100000, loss: 0.000523, epsilon: 0.010006, episode:  249\n",
      "frames: 361000, reward: 12.100000, loss: 0.002290, epsilon: 0.010006, episode:  249\n",
      "frames: 362000, reward: 13.300000, loss: 0.004315, epsilon: 0.010006, episode:  250\n",
      "frames: 363000, reward: 13.300000, loss: 0.000589, epsilon: 0.010006, episode:  250\n",
      "frames: 364000, reward: 13.300000, loss: 0.001902, epsilon: 0.010005, episode:  250\n",
      "frames: 365000, reward: 12.300000, loss: 0.000345, epsilon: 0.010005, episode:  251\n",
      "frames: 366000, reward: 12.300000, loss: 0.000495, epsilon: 0.010005, episode:  251\n",
      "frames: 367000, reward: 12.300000, loss: 0.000577, epsilon: 0.010005, episode:  251\n",
      "frames: 368000, reward: 11.600000, loss: 0.001190, epsilon: 0.010005, episode:  252\n",
      "frames: 369000, reward: 11.600000, loss: 0.000510, epsilon: 0.010005, episode:  252\n",
      "frames: 370000, reward: 11.600000, loss: 0.000538, epsilon: 0.010004, episode:  252\n",
      "frames: 371000, reward: 11.800000, loss: 0.000773, epsilon: 0.010004, episode:  253\n",
      "frames: 372000, reward: 11.800000, loss: 0.000272, epsilon: 0.010004, episode:  253\n",
      "frames: 373000, reward: 13.200000, loss: 0.000316, epsilon: 0.010004, episode:  254\n",
      "frames: 374000, reward: 13.100000, loss: 0.000170, epsilon: 0.010004, episode:  255\n",
      "frames: 375000, reward: 13.100000, loss: 0.000389, epsilon: 0.010004, episode:  255\n",
      "frames: 376000, reward: 13.100000, loss: 0.000250, epsilon: 0.010004, episode:  255\n",
      "frames: 377000, reward: 13.000000, loss: 0.000183, epsilon: 0.010003, episode:  256\n",
      "frames: 378000, reward: 13.000000, loss: 0.000358, epsilon: 0.010003, episode:  256\n",
      "frames: 379000, reward: 14.000000, loss: 0.000331, epsilon: 0.010003, episode:  257\n",
      "frames: 380000, reward: 14.000000, loss: 0.000632, epsilon: 0.010003, episode:  257\n",
      "frames: 381000, reward: 16.000000, loss: 0.000316, epsilon: 0.010003, episode:  258\n",
      "frames: 382000, reward: 16.000000, loss: 0.000210, epsilon: 0.010003, episode:  258\n",
      "frames: 383000, reward: 16.400000, loss: 0.000455, epsilon: 0.010003, episode:  259\n",
      "frames: 384000, reward: 16.400000, loss: 0.000673, epsilon: 0.010003, episode:  259\n",
      "frames: 385000, reward: 16.300000, loss: 0.000198, epsilon: 0.010003, episode:  260\n",
      "frames: 386000, reward: 16.300000, loss: 0.000542, epsilon: 0.010003, episode:  260\n",
      "frames: 387000, reward: 16.300000, loss: 0.000804, epsilon: 0.010002, episode:  260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames: 388000, reward: 16.300000, loss: 0.001164, epsilon: 0.010002, episode:  260\n",
      "frames: 389000, reward: 16.400000, loss: 0.000550, epsilon: 0.010002, episode:  261\n",
      "frames: 390000, reward: 16.400000, loss: 0.000950, epsilon: 0.010002, episode:  261\n",
      "frames: 391000, reward: 16.400000, loss: 0.000297, epsilon: 0.010002, episode:  261\n",
      "frames: 392000, reward: 16.700000, loss: 0.000941, epsilon: 0.010002, episode:  262\n",
      "frames: 393000, reward: 16.700000, loss: 0.000534, epsilon: 0.010002, episode:  262\n",
      "frames: 394000, reward: 16.300000, loss: 0.000831, epsilon: 0.010002, episode:  263\n",
      "frames: 395000, reward: 16.300000, loss: 0.000444, epsilon: 0.010002, episode:  263\n",
      "frames: 396000, reward: 16.300000, loss: 0.000486, epsilon: 0.010002, episode:  263\n",
      "frames: 397000, reward: 15.800000, loss: 0.000484, epsilon: 0.010002, episode:  264\n",
      "frames: 398000, reward: 15.800000, loss: 0.000732, epsilon: 0.010002, episode:  264\n",
      "frames: 399000, reward: 15.800000, loss: 0.000753, epsilon: 0.010002, episode:  264\n",
      "frames: 400000, reward: 15.100000, loss: 0.001117, epsilon: 0.010002, episode:  265\n",
      "frames: 401000, reward: 15.100000, loss: 0.000831, epsilon: 0.010002, episode:  265\n",
      "frames: 402000, reward: 15.100000, loss: 0.000223, epsilon: 0.010001, episode:  265\n",
      "frames: 403000, reward: 15.100000, loss: 0.000256, epsilon: 0.010001, episode:  265\n",
      "frames: 404000, reward: 14.300000, loss: 0.000291, epsilon: 0.010001, episode:  266\n",
      "frames: 405000, reward: 14.300000, loss: 0.000648, epsilon: 0.010001, episode:  266\n",
      "frames: 406000, reward: 14.300000, loss: 0.004684, epsilon: 0.010001, episode:  266\n",
      "frames: 407000, reward: 14.300000, loss: 0.002102, epsilon: 0.010001, episode:  266\n",
      "frames: 408000, reward: 14.300000, loss: 0.000193, epsilon: 0.010001, episode:  266\n",
      "frames: 409000, reward: 13.200000, loss: 0.002809, epsilon: 0.010001, episode:  267\n",
      "frames: 410000, reward: 13.200000, loss: 0.000756, epsilon: 0.010001, episode:  267\n",
      "frames: 411000, reward: 13.100000, loss: 0.000766, epsilon: 0.010001, episode:  268\n",
      "frames: 412000, reward: 13.100000, loss: 0.001596, epsilon: 0.010001, episode:  268\n",
      "frames: 413000, reward: 11.900000, loss: 0.000876, epsilon: 0.010001, episode:  269\n",
      "frames: 414000, reward: 11.900000, loss: 0.000638, epsilon: 0.010001, episode:  269\n",
      "frames: 415000, reward: 11.900000, loss: 0.000950, epsilon: 0.010001, episode:  269\n",
      "frames: 416000, reward: 11.500000, loss: 0.000874, epsilon: 0.010001, episode:  270\n",
      "frames: 417000, reward: 12.900000, loss: 0.000649, epsilon: 0.010001, episode:  271\n",
      "frames: 418000, reward: 12.900000, loss: 0.000483, epsilon: 0.010001, episode:  271\n",
      "frames: 419000, reward: 13.100000, loss: 0.001030, epsilon: 0.010001, episode:  272\n",
      "frames: 420000, reward: 13.100000, loss: 0.000357, epsilon: 0.010001, episode:  272\n",
      "frames: 421000, reward: 13.100000, loss: 0.000760, epsilon: 0.010001, episode:  272\n",
      "frames: 422000, reward: 13.100000, loss: 0.000357, epsilon: 0.010001, episode:  272\n",
      "frames: 423000, reward: 12.800000, loss: 0.000656, epsilon: 0.010001, episode:  273\n",
      "frames: 424000, reward: 12.800000, loss: 0.000354, epsilon: 0.010001, episode:  273\n",
      "frames: 425000, reward: 13.100000, loss: 0.000339, epsilon: 0.010001, episode:  274\n",
      "frames: 426000, reward: 13.100000, loss: 0.001755, epsilon: 0.010001, episode:  274\n",
      "frames: 427000, reward: 14.000000, loss: 0.000472, epsilon: 0.010001, episode:  275\n",
      "frames: 428000, reward: 14.000000, loss: 0.000411, epsilon: 0.010001, episode:  275\n",
      "frames: 429000, reward: 15.400000, loss: 0.000612, epsilon: 0.010001, episode:  276\n",
      "frames: 430000, reward: 15.400000, loss: 0.000249, epsilon: 0.010001, episode:  276\n",
      "frames: 431000, reward: 16.500000, loss: 0.000787, epsilon: 0.010001, episode:  277\n",
      "frames: 432000, reward: 16.500000, loss: 0.000264, epsilon: 0.010001, episode:  277\n",
      "frames: 433000, reward: 16.700000, loss: 0.000191, epsilon: 0.010001, episode:  278\n",
      "frames: 434000, reward: 16.700000, loss: 0.000322, epsilon: 0.010001, episode:  278\n",
      "frames: 435000, reward: 16.700000, loss: 0.000285, epsilon: 0.010000, episode:  278\n",
      "frames: 436000, reward: 16.700000, loss: 0.004250, epsilon: 0.010000, episode:  278\n",
      "frames: 437000, reward: 16.800000, loss: 0.000560, epsilon: 0.010000, episode:  279\n",
      "frames: 438000, reward: 16.800000, loss: 0.000649, epsilon: 0.010000, episode:  279\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__':\n",
    "    \n",
    "# Training DQN in PongNoFrameskip-v4 \n",
    "env = make_atari('PongNoFrameskip-v4')\n",
    "env = wrap_deepmind(env, scale = False, frame_stack=True)\n",
    "\n",
    "gamma = 0.99\n",
    "epsilon_max = 1\n",
    "epsilon_min = 0.01\n",
    "eps_decay = 30000\n",
    "frames = 2000000\n",
    "USE_CUDA = False\n",
    "learning_rate = 2e-4\n",
    "max_buff = 100000\n",
    "update_tar_interval = 1000\n",
    "batch_size = 32\n",
    "print_interval = 1000\n",
    "log_interval = 1000\n",
    "learning_start = 10000\n",
    "win_reward = 18     # Pong-v4\n",
    "win_break = True\n",
    "\n",
    "action_space = env.action_space\n",
    "action_dim = env.action_space.n\n",
    "state_dim = env.observation_space.shape[0]\n",
    "state_channel = env.observation_space.shape[2]\n",
    "agent = DQNAgent(in_channels = state_channel, action_space= action_space, USE_CUDA = USE_CUDA, lr = learning_rate)\n",
    "\n",
    "frame = env.reset()\n",
    "\n",
    "episode_reward = 0\n",
    "all_rewards = []\n",
    "losses = []\n",
    "episode_num = 0\n",
    "is_win = False\n",
    "# tensorboard\n",
    "summary_writer = SummaryWriter(log_dir = \"DQN_stackframe\", comment= \"good_makeatari\")\n",
    "\n",
    "# e-greedy decay\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_min + (epsilon_max - epsilon_min) * math.exp(\n",
    "            -1. * frame_idx / eps_decay)\n",
    "# plt.plot([epsilon_by_frame(i) for i in range(10000)])\n",
    "\n",
    "for i in range(frames):\n",
    "    epsilon = epsilon_by_frame(i)\n",
    "    state_tensor = agent.observe(frame)\n",
    "    action = agent.act(state_tensor, epsilon)\n",
    "    \n",
    "    next_frame, reward, done, _ = env.step(action)\n",
    "    \n",
    "    episode_reward += reward\n",
    "    agent.memory_buffer.push(frame, action, reward, next_frame, done)\n",
    "    frame = next_frame\n",
    "    \n",
    "    loss = 0\n",
    "    if agent.memory_buffer.size() >= learning_start:\n",
    "        loss = agent.learn_from_experience(batch_size)\n",
    "        losses.append(loss)\n",
    "\n",
    "    if i % print_interval == 0:\n",
    "        print(\"frames: %5d, reward: %5f, loss: %4f, epsilon: %5f, episode: %4d\" % (i, np.mean(all_rewards[-10:]), loss, epsilon, episode_num))\n",
    "        summary_writer.add_scalar(\"Temporal Difference Loss\", loss, i)\n",
    "        summary_writer.add_scalar(\"Mean Reward\", np.mean(all_rewards[-10:]), i)\n",
    "        summary_writer.add_scalar(\"Epsilon\", epsilon, i)\n",
    "        \n",
    "    if i % update_tar_interval == 0:\n",
    "        agent.DQN_target.load_state_dict(agent.DQN.state_dict())\n",
    "    \n",
    "    if done:\n",
    "        \n",
    "        frame = env.reset()\n",
    "        \n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        episode_num += 1\n",
    "        avg_reward = float(np.mean(all_rewards[-100:]))\n",
    "\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(frame_idx, rewards, losses):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)\n",
    "    plt.show()\n",
    "\n",
    "plot_training(i, all_rewards, losses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Establish a baseline performance. How well did your Deep Q-learning do on your problem? (5 Points)\n",
    "total_episodes = 2000\n",
    "tital_test_episodes = 200\n",
    "max_steps = 2000000\n",
    "learning rate = 2e-4\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "eps_decay = 30000\n",
    "2. What are the states, the actions, and the size of the Q-table? (5 Points)\n",
    "The states are the position of the platform in the current frame. The actions are the platform moving up or down. The size og Q-table is the product of all the states and the actions.\n",
    "3. What are the rewards? Why did you choose them? (5 Points)\n",
    "A reward is awarded when the plaform catches the ball. Because the goal of the training is to make sure agent doesn't lose and wins by 21 goals.\n",
    "4. How did you choose alpha and gamma in the Bellman equation? Try at least one additional value for alpha and gamma. How did it change the baseline performance?  (5 Points)\n",
    "I'm trying to make the future more important, so I set the gamma at 0.99. Gamma usually has a value between 0 and 1, the smaller the value, the less far-sighted it is.\n",
    "5. Try a policy other than maxQ(s', a'). How did it change the baseline performance? (5 Points)\n",
    "The efficiency of training may decrease without affecting the final result. Because changing the policy only affects the current reward.\n",
    "6. How did you choose your decay rate and starting epsilon? Try at least one additional value for epsilon and the decay rate. How did it change the baseline performance? What is the value of epsilon when if you reach the max steps per episode? (5 Points)\n",
    "The value of epsilon represents the exploration. When the number of steps reaches the maximum, epsilon is infinitely close to the minimum value of 0.01.\n",
    "7. What is the average number of steps taken per episode? (5 Points)\n",
    "1000\n",
    "8. Does Q-learning use value-based or policy-based iteration? (5 Points)\n",
    "Value-based. Because in Q-learning, we need to get every maxQ(s,a) value and using them as next basic value.\n",
    "9. Could you use SARSA for this problem? (5 Points)\n",
    "no. I don't fully understand this code yet. I just got a general idea of how SARSA works.\n",
    "10. What is meant by the expected lifetime value in the Bellman equation?(5 Points)\n",
    "In the bellman equation, the large the gamma value, the more focused on long-term returns.\n",
    "11. When would SARSA likely do better than Q-learning? (5 Points)\n",
    "SARSA avoids the cliffs in favor of a safer path. Q-learning might fall off a cliff in search of a locally optimal solution.\n",
    "12. How does SARSA differ from Q-learning? (5 Points) \n",
    "SARSA avoids the cliffs in favor of a safer path. Q-learning might fall off a cliff in search of a locally optimal solution.\n",
    "13. Explain the Q-learning algorithm. (5 Points)\n",
    "Initialize Q(s,a) arbitrarily\n",
    "Repeat(for each episode):\n",
    "  Initialize s\n",
    "  Repeat(for each step of episode):\n",
    "   Choose a form s using policy derived from Q (e.g., epsilon-greedy)\n",
    "   Take action a, observe r,s’\n",
    "Q(s,a) <--Q(s,a)+alpha[r+gamma max(x) a’ Q(s’,a’)-Q(s,a)]\n",
    "  s <- s’;\n",
    "Until s is terminal\n",
    "14. Explain the SARSA algorithm. (5 Points)  \n",
    "Initialize Q(s,a) arbitrarily\n",
    "Repeat(for each episode):\n",
    "  Initialize s\n",
    "  Repeat(for each step of episode):\n",
    "   Choose a form s using policy derived from Q (e.g., epsilon-greedy)\n",
    "   Take action a, observe r,s’\n",
    "Q(s,a) <--Q(s,a)+alpha[r+gamma Q(s’,a’)-Q(s,a)]\n",
    "  s <- s’; a<-a’\n",
    "Until s is terminal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
